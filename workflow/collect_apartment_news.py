"""
ì•„íŒŒíŠ¸ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° LLM ê¸°ë°˜ ê´€ë ¨ì„± íŒë‹¨ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. BigQueryì—ì„œ ì•„íŒŒíŠ¸ ëª©ë¡ì„ ì¡°íšŒ
2. ë„¤ì´ë²„ ê²€ìƒ‰ APIë¡œ ê° ì•„íŒŒíŠ¸ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
3. LLM(OpenAI/Claude)ì„ ì‚¬ìš©í•´ ê´€ë ¨ì„± íŒë‹¨
4. ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ë§Œ í•„í„°ë§í•˜ì—¬ JSON ì €ì¥

Usage:
    python workflow/collect_apartment_news.py

Environment Variables:
    - NAVER_CLIENT_ID: ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ID
    - NAVER_CLIENT_SECRET: ë„¤ì´ë²„ API ì‹œí¬ë¦¿
    - OPENAI_API_KEY: OpenAI API í‚¤ (ê´€ë ¨ì„± íŒë‹¨ìš©)
    - GOOGLE_APPLICATION_CREDENTIALS: BigQuery ì¸ì¦ (ì„ íƒ)
"""

import json
import os
import re
import urllib.parse
import urllib.request
from dataclasses import asdict, dataclass
from datetime import datetime

# ============================================
# ì„¤ì •
# ============================================

OUTPUT_PATH = "data/apartment_news.json"
NEWS_PER_APARTMENT = 10  # ì•„íŒŒíŠ¸ë‹¹ ìˆ˜ì§‘í•  ë‰´ìŠ¤ ìˆ˜
MIN_RELEVANCE_SCORE = 0.6  # ê´€ë ¨ì„± ì ìˆ˜ ì„ê³„ê°’ (0~1)

# ì£¼ìš” ì•„íŒŒíŠ¸ ëª©ë¡ (BigQuery ì¡°íšŒ ëŒ€ì‹  ì§ì ‘ ì§€ì •ë„ ê°€ëŠ¥)
TARGET_APARTMENTS = [
    {"name": "í—¬ë¦¬ì˜¤ì‹œí‹°", "region": "ê°€ë½ë™"},
    {"name": "ë˜ë¯¸ì•ˆë¸”ë ˆìŠ¤í‹°ì§€", "region": "ê°œí¬ë™"},
    {"name": "ë˜ë¯¸ì•ˆëŒ€ì¹˜íŒ°ë¦¬ìŠ¤", "region": "ëŒ€ì¹˜ë™"},
    {"name": "ë¦¬ì„¼ì¸ ", "region": "ì ì‹¤ë™"},
    {"name": "ë°˜í¬ìì´", "region": "ë°˜í¬ë™"},
    {"name": "ë˜ë¯¸ì•ˆì›ë² ì¼ë¦¬", "region": "ë°˜í¬ë™"},
    {"name": "ì ì‹¤ì—˜ìŠ¤", "region": "ì ì‹¤ë™"},
    {"name": "ì•„í¬ë¡œë¦¬ë²„íŒŒí¬", "region": "ë°˜í¬ë™"},
    {"name": "íŠ¸ë¦¬ì§€ì›€", "region": "ì ì‹¤ë™"},
    {"name": "ë˜ë¯¸ì•ˆí¼ìŠ¤í‹°ì§€", "region": "ë°˜í¬ë™"},
]


# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================


def clean_html(text: str) -> str:
    """HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°, JSON ì•ˆì „ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    clean = re.sub(r"<[^>]+>", "", text)
    clean = clean.replace("&quot;", '"')
    clean = clean.replace("&amp;", "&")
    clean = clean.replace("&lt;", "<")
    clean = clean.replace("&gt;", ">")
    clean = clean.replace("&apos;", "'")
    # JSON ë¬¸ìì—´ ë‚´ í°ë”°ì˜´í‘œ â†’ ì‘ì€ë”°ì˜´í‘œ
    clean = re.sub(r'"([^"]+)"', r"'\1'", clean)
    clean = clean.replace('"', "'")
    return clean.strip()


def parse_date(pub_date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        from email.utils import parsedate_to_datetime

        dt = parsedate_to_datetime(pub_date_str)
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return pub_date_str[:10] if len(pub_date_str) >= 10 else pub_date_str


def extract_source(link: str) -> str:
    """ë§í¬ì—ì„œ ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ì¶œ"""
    source_mapping = {
        "hankyung.com": "í•œêµ­ê²½ì œ",
        "sedaily.com": "ì„œìš¸ê²½ì œ",
        "newsis.com": "ë‰´ì‹œìŠ¤",
        "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
        "mk.co.kr": "ë§¤ì¼ê²½ì œ",
        "chosun.com": "ì¡°ì„ ì¼ë³´",
        "donga.com": "ë™ì•„ì¼ë³´",
        "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
        "hani.co.kr": "í•œê²¨ë ˆ",
        "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
        "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
        "sbs.co.kr": "SBS",
        "kbs.co.kr": "KBS",
        "mbc.co.kr": "MBC",
        "etoday.co.kr": "ì´íˆ¬ë°ì´",
        "newspim.com": "ë‰´ìŠ¤í•Œ",
        "moneys.co.kr": "ë¨¸ë‹ˆS",
        "bizhankook.com": "ë¹„ì¦ˆí•œêµ­",
    }
    for domain, source in source_mapping.items():
        if domain in link:
            return source
    return "ê¸°íƒ€"


# ============================================
# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
# ============================================


def search_naver_news(query: str, display: int = 10) -> dict:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í˜¸ì¶œ"""
    client_id = os.environ.get("NAVER_CLIENT_ID")
    client_secret = os.environ.get("NAVER_CLIENT_SECRET")

    if not client_id or not client_secret:
        print("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {"items": [], "total": 0}

    encoded_query = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&sort=date"

    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", client_id)
    request.add_header("X-Naver-Client-Secret", client_secret)

    try:
        response = urllib.request.urlopen(request)
        return json.loads(response.read().decode("utf-8"))
    except Exception as e:
        print(f"   âŒ ë„¤ì´ë²„ API ì˜¤ë¥˜: {e}")
        return {"items": [], "total": 0}


# ============================================
# LLM ê¸°ë°˜ ê´€ë ¨ì„± íŒë‹¨
# ============================================


def judge_relevance_with_llm(apartment_name: str, news_title: str, news_desc: str) -> dict:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨

    Returns:
        dict: {"score": 0.0~1.0, "reason": "íŒë‹¨ ê·¼ê±°"}
    """
    api_key = os.environ.get("OPENAI_API_KEY")

    if not api_key:
        # API í‚¤ê°€ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ íŒë‹¨
        return judge_relevance_simple(apartment_name, news_title, news_desc)

    try:
        import openai

        client = openai.OpenAI(api_key=api_key)

        prompt = f"""ë‹¹ì‹ ì€ ë¶€ë™ì‚° ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ë‰´ìŠ¤ê°€ '{apartment_name}' ì•„íŒŒíŠ¸ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

[ë‰´ìŠ¤ ì œëª©]
{news_title}

[ë‰´ìŠ¤ ë‚´ìš©]
{news_desc}

íŒë‹¨ ê¸°ì¤€:
- í•´ë‹¹ ì•„íŒŒíŠ¸ê°€ ì§ì ‘ ì–¸ê¸‰ë˜ì–´ êµ¬ì²´ì ì¸ ì •ë³´(ê°€ê²©, ê±°ë˜, ì¬ê±´ì¶•, ë¶„ì–‘ ë“±)ê°€ ìˆìœ¼ë©´ ê´€ë ¨ì„± ë†’ìŒ (0.8~1.0)
- í•´ë‹¹ ì•„íŒŒíŠ¸ê°€ ì–¸ê¸‰ë˜ì§€ë§Œ ë‹¨ìˆœ ë‚˜ì—´ì— ë¶ˆê³¼í•˜ë©´ ì¤‘ê°„ (0.5~0.7)
- ë™ì¼ ì§€ì—­ì´ë‚˜ ìœ ì‚¬ ì•„íŒŒíŠ¸ë§Œ ì–¸ê¸‰ë˜ë©´ ë‚®ìŒ (0.3~0.5)
- ì „í˜€ ê´€ë ¨ ì—†ìœ¼ë©´ ë§¤ìš° ë‚®ìŒ (0~0.3)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{"score": 0.0~1.0, "reason": "íŒë‹¨ ê·¼ê±° í•œ ì¤„ ìš”ì•½"}}"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            max_tokens=150,
            temperature=0.1,
        )

        result = json.loads(response.choices[0].message.content)
        return result

    except Exception as e:
        print(f"   âš ï¸ LLM íŒë‹¨ ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´: {e}")
        return judge_relevance_simple(apartment_name, news_title, news_desc)


def judge_relevance_simple(apartment_name: str, news_title: str, news_desc: str) -> dict:
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ê´€ë ¨ì„± íŒë‹¨ (LLM ì—†ì´)
    """
    combined = f"{news_title} {news_desc}".lower()
    apt_lower = apartment_name.lower()

    # ì§ì ‘ ì–¸ê¸‰ ì²´í¬
    if apt_lower in combined:
        # êµ¬ì²´ì  ì •ë³´ ì²´í¬ (ê°€ê²©, ê±°ë˜ ë“±)
        price_keywords = ["ì–µ", "ë§Œì›", "ê±°ë˜", "ì‹ ê³ ê°€", "ìµœê³ ê°€", "ë§¤ë§¤", "ì „ì„¸"]
        has_price_info = any(kw in combined for kw in price_keywords)

        if has_price_info:
            return {"score": 0.9, "reason": "ì§ì ‘ ì–¸ê¸‰ + ê°€ê²©/ê±°ë˜ ì •ë³´"}
        else:
            return {"score": 0.7, "reason": "ì§ì ‘ ì–¸ê¸‰"}

    # ë‹¨ì§€ëª… ì¼ë¶€ ë§¤ì¹­
    apt_parts = apt_lower.replace("ë˜ë¯¸ì•ˆ", "").replace("ìì´", "").replace("íìŠ¤í…Œì´íŠ¸", "")
    if len(apt_parts) > 2 and apt_parts in combined:
        return {"score": 0.5, "reason": "ë¶€ë¶„ ë§¤ì¹­"}

    return {"score": 0.2, "reason": "ê´€ë ¨ì„± ë‚®ìŒ"}


# ============================================
# ë©”ì¸ ë¡œì§
# ============================================


@dataclass
class NewsItem:
    title: str
    link: str
    description: str
    pubDate: str
    source: str
    relevance: str
    relevance_score: float


def collect_apartment_news(apartment: dict, use_llm: bool = True) -> dict:
    """ë‹¨ì¼ ì•„íŒŒíŠ¸ì— ëŒ€í•œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° í•„í„°ë§"""
    apt_name = apartment["name"]
    region = apartment["region"]

    print(f"\nğŸ” [{apt_name}] ({region}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

    # ë‰´ìŠ¤ ê²€ìƒ‰
    search_result = search_naver_news(f"{apt_name} ì•„íŒŒíŠ¸", NEWS_PER_APARTMENT)

    if not search_result.get("items"):
        print("   âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, ì§€ì—­ëª…ìœ¼ë¡œ ì¬ê²€ìƒ‰...")
        search_result = search_naver_news(f"{region} {apt_name}", NEWS_PER_APARTMENT)

    total_news = search_result.get("total", 0)
    raw_items = search_result.get("items", [])

    print(f"   ğŸ“° ì´ {total_news:,}ê±´ ì¤‘ {len(raw_items)}ê±´ ë¶„ì„...")

    # ê´€ë ¨ì„± íŒë‹¨ ë° í•„í„°ë§
    filtered_items = []

    for item in raw_items:
        title = clean_html(item.get("title", ""))
        desc = clean_html(item.get("description", ""))

        # ê´€ë ¨ì„± íŒë‹¨
        if use_llm:
            relevance = judge_relevance_with_llm(apt_name, title, desc)
        else:
            relevance = judge_relevance_simple(apt_name, title, desc)

        score = relevance.get("score", 0)
        reason = relevance.get("reason", "")

        if score >= MIN_RELEVANCE_SCORE:
            news_item = NewsItem(
                title=title,
                link=item.get("link", ""),
                description=desc,
                pubDate=parse_date(item.get("pubDate", "")),
                source=extract_source(item.get("originallink", item.get("link", ""))),
                relevance=reason,
                relevance_score=score,
            )
            filtered_items.append(asdict(news_item))
            print(f"   âœ… [{score:.1f}] {title[:40]}...")
        else:
            print(f"   âŒ [{score:.1f}] {title[:40]}... (í•„í„°ë§)")

    # ê´€ë ¨ì„± ì ìˆ˜ìˆœ ì •ë ¬
    filtered_items.sort(key=lambda x: x["relevance_score"], reverse=True)

    # relevance_score í•„ë“œ ì œê±° (ì¶œë ¥ìš©)
    for item in filtered_items:
        del item["relevance_score"]

    # ìš”ì•½ ìƒì„±
    summary = generate_summary(apt_name, filtered_items)

    # ê´€ë ¨ë„ ë“±ê¸‰ ì‚°ì •
    avg_score = (
        sum(judge_relevance_simple(apt_name, item["title"], item["description"])["score"] for item in filtered_items)
        / len(filtered_items)
        if filtered_items
        else 0
    )
    relevance_level = "very_high" if avg_score >= 0.85 else "high" if avg_score >= 0.7 else "medium"

    return {
        "region": region,
        "total_news": total_news,
        "relevance_score": relevance_level,
        "news_count": len(filtered_items),
        "summary": summary,
        "items": filtered_items,
    }


def generate_summary(apt_name: str, items: list) -> str:
    """ë‰´ìŠ¤ ì•„ì´í…œë“¤ë¡œë¶€í„° ìš”ì•½ ìƒì„±"""
    if not items:
        return f"{apt_name} ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = []
    all_text = " ".join([item["title"] + " " + item["description"] for item in items])

    if any(kw in all_text for kw in ["ì‹ ê³ ê°€", "ìµœê³ ê°€", "ì–µì›"]):
        keywords.append("ì‹ ê³ ê°€ ê²½ì‹ ")
    if any(kw in all_text for kw in ["ìƒìŠ¹", "ê¸‰ë“±", "ì˜¤ë¥´"]):
        keywords.append("ê°€ê²© ìƒìŠ¹")
    if any(kw in all_text for kw in ["ì¬ê±´ì¶•", "ì¬ê°œë°œ", "ì •ë¹„"]):
        keywords.append("ì¬ê±´ì¶•")
    if any(kw in all_text for kw in ["ë¶„ì–‘", "ì²­ì•½", "ì…ì£¼"]):
        keywords.append("ë¶„ì–‘/ì…ì£¼")
    if any(kw in all_text for kw in ["ê±°ë˜ëŸ‰", "ë§¤ë¬¼"]):
        keywords.append("ê±°ë˜ ë™í–¥")

    if keywords:
        return f"{apt_name}: {', '.join(keywords[:3])} ê´€ë ¨ ë‰´ìŠ¤ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤."
    else:
        return f"{apt_name} ê´€ë ¨ ìµœì‹  ë¶€ë™ì‚° ë‰´ìŠ¤ì…ë‹ˆë‹¤."


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ¢ ì•„íŒŒíŠ¸ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ (LLM ê´€ë ¨ì„± íŒë‹¨)")
    print("=" * 60)

    # LLM ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_llm = bool(os.environ.get("OPENAI_API_KEY"))
    if use_llm:
        print("âœ… OpenAI API í‚¤ ê°ì§€ë¨ - LLM ê¸°ë°˜ ê´€ë ¨ì„± íŒë‹¨ í™œì„±í™”")
    else:
        print("âš ï¸ OpenAI API í‚¤ ì—†ìŒ - í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ ì‚¬ìš©")

    # ê²°ê³¼ ìˆ˜ì§‘
    result = {
        "last_updated": datetime.now().isoformat(),
        "metadata": {
            "total_apartments": len(TARGET_APARTMENTS),
            "relevance_filter": "LLM-based" if use_llm else "keyword-based",
            "min_relevance_score": MIN_RELEVANCE_SCORE,
            "description": "ì•„íŒŒíŠ¸ë³„ ë‰´ìŠ¤ (ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ë§Œ í•„í„°ë§)",
        },
        "apartments": {},
        "fallback_regions": {
            "description": "ì•„íŒŒíŠ¸ë³„ ë‰´ìŠ¤ê°€ ë¶€ì¡±í•œ ê²½ìš° ë™ ë‹¨ìœ„ ë‰´ìŠ¤ í‘œì‹œ",
            "regions": list(set(apt["region"] for apt in TARGET_APARTMENTS)),
        },
    }

    for apartment in TARGET_APARTMENTS:
        apt_data = collect_apartment_news(apartment, use_llm=use_llm)
        result["apartments"][apartment["name"]] = apt_data

    # ì €ì¥
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 60)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_PATH}")
    print(f"ğŸ“Š ì´ {len(result['apartments'])}ê°œ ì•„íŒŒíŠ¸ ìˆ˜ì§‘")

    # í†µê³„ ì¶œë ¥
    total_news = sum(apt["news_count"] for apt in result["apartments"].values())
    print(f"ğŸ“° ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ ì´ {total_news}ê±´ ìˆ˜ì§‘")


if __name__ == "__main__":
    main()
