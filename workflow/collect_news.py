"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í†µí•´ ì§€ì—­ë³„ ë¶€ë™ì‚° ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³ 
JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

Usage:
    python workflow/collect_news.py

Note:
    - ë„¤ì´ë²„ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤ (NAVER_CLIENT_ID, NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜)
    - MCP ì„œë²„ë¥¼ í†µí•´ ìˆ˜ì§‘í•˜ëŠ” ê²½ìš° ì´ ìŠ¤í¬ë¦½íŠ¸ ëŒ€ì‹  MCP ë„êµ¬ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì„¸ìš”.
"""

import json
import os
import re
from datetime import datetime

# ì§€ì—­ ëª©ë¡ (BigQueryì—ì„œ ì¡°íšŒí•œ ì£¼ìš” ì§€ì—­ë“¤)
TARGET_REGIONS = [
    "ì ì‹¤ë™",
    "ê°œí¬ë™",
    "ëŒ€ì¹˜ë™",
    "ë°˜í¬ë™",
    "ì„œì´ˆë™",
    "ë‹¹ì‚°ë™",
    "ì—¬ì˜ë„ë™",
    "ëª©ë™",
    "ë´‰ì²œë™",
    "ì‹ ë¦¼ë™",
    "ì´ì´Œë™",
    "ì˜ë“±í¬ë™",
]

# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
OUTPUT_PATH = "data/news_headlines.json"


def clean_html_tags(text: str) -> str:
    """HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°, JSON ì•ˆì „ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    # HTML íƒœê·¸ ì œê±°
    clean = re.sub(r"<[^>]+>", "", text)
    # &quot; ë“± HTML ì—”í‹°í‹° ë³€í™˜
    clean = clean.replace("&quot;", '"')
    clean = clean.replace("&amp;", "&")
    clean = clean.replace("&lt;", "<")
    clean = clean.replace("&gt;", ">")
    clean = clean.replace("&apos;", "'")

    # JSON ë¬¸ìì—´ ë‚´ í°ë”°ì˜´í‘œ ì²˜ë¦¬ (ê²¹ë”°ì˜´í‘œ â†’ ì‘ì€ë”°ì˜´í‘œ)
    # ì˜ˆ: "ê²°êµ­ì€ ì§‘ê°’ ì˜¤ë¥¼ ê²ƒ" â†’ 'ê²°êµ­ì€ ì§‘ê°’ ì˜¤ë¥¼ ê²ƒ'
    clean = re.sub(r'"([^"]+)"', r"'\1'", clean)

    # í˜¹ì‹œ ë‚¨ì•„ìˆëŠ” ë‹¨ë… í°ë”°ì˜´í‘œë„ ì‘ì€ë”°ì˜´í‘œë¡œ ë³€í™˜
    clean = clean.replace('"', "'")

    return clean.strip()


def extract_source_from_link(link: str) -> str:
    """ë§í¬ì—ì„œ ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ì¶œ"""
    source_mapping = {
        "hankyung.com": "í•œêµ­ê²½ì œ",
        "sedaily.com": "ì„œìš¸ê²½ì œ",
        "newsis.com": "ë‰´ì‹œìŠ¤",
        "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
        "mk.co.kr": "ë§¤ì¼ê²½ì œ",
        "chosun.com": "ì¡°ì„ ì¼ë³´",
        "donga.com": "ë™ì•„ì¼ë³´",
        "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
        "hani.co.kr": "í•œê²¨ë ˆ",
        "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
        "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
        "sbs.co.kr": "SBS",
        "kbs.co.kr": "KBS",
        "mbc.co.kr": "MBC",
        "ichannela.com": "ì±„ë„A",
        "inews24.com": "ì•„ì´ë‰´ìŠ¤24",
        "newspim.com": "ë‰´ìŠ¤í•Œ",
        "moneys.co.kr": "ë¨¸ë‹ˆS",
        "pinpointnews.co.kr": "í•€í¬ì¸íŠ¸ë‰´ìŠ¤",
        "areyou.co.kr": "ì•„ìœ ê²½ì œ",
        "munhwa.com": "ë¬¸í™”ì¼ë³´",
        "mediapen.com": "ë¯¸ë””ì–´íœ",
    }

    for domain, source in source_mapping.items():
        if domain in link:
            return source
    return "ê¸°íƒ€"


def format_pub_date(pub_date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # "Sun, 07 Dec 2025 19:32:00 +0900" í˜•ì‹
        from email.utils import parsedate_to_datetime

        dt = parsedate_to_datetime(pub_date_str)
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return pub_date_str[:10] if len(pub_date_str) >= 10 else pub_date_str


def create_sample_news_structure() -> dict:
    """
    ìƒ˜í”Œ ë‰´ìŠ¤ ë°ì´í„° êµ¬ì¡° ìƒì„±

    ì‹¤ì œë¡œëŠ” ë„¤ì´ë²„ APIë‚˜ MCPë¥¼ í†µí•´ ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œ ì±„ì›Œì•¼ í•©ë‹ˆë‹¤.
    """
    return {"last_updated": datetime.now().isoformat(), "regions": {}}


def generate_summary(items: list, region: str) -> str:
    """ë‰´ìŠ¤ ì•„ì´í…œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ ìƒì„± (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)"""
    if not items:
        return f"{region} ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

    # í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©ì—ì„œ)
    keywords = []
    for item in items[:3]:
        title = item.get("title", "")
        # ê°€ê²© ê´€ë ¨
        if any(kw in title for kw in ["ì–µ", "ìƒìŠ¹", "ì˜¤ë¥´", "ê¸‰ë“±"]):
            keywords.append("ê°€ê²© ìƒìŠ¹")
        if any(kw in title for kw in ["í•˜ë½", "ë–¨ì–´", "ê¸‰ë½"]):
            keywords.append("ê°€ê²© í•˜ë½")
        # ì¬ê±´ì¶• ê´€ë ¨
        if any(kw in title for kw in ["ì¬ê±´ì¶•", "ì¬ê°œë°œ", "ì •ë¹„"]):
            keywords.append("ì¬ê±´ì¶•")
        # ë¶„ì–‘ ê´€ë ¨
        if any(kw in title for kw in ["ë¶„ì–‘", "ì²­ì•½", "ì…ì£¼"]):
            keywords.append("ë¶„ì–‘/ì…ì£¼")

    keywords = list(set(keywords))
    if keywords:
        return f"{region}ì€(ëŠ”) {', '.join(keywords)} ê´€ë ¨ ë‰´ìŠ¤ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤."
    else:
        return f"{region} ê´€ë ¨ ìµœì‹  ë¶€ë™ì‚° ë‰´ìŠ¤ì…ë‹ˆë‹¤."


def process_news_item(item: dict) -> dict:
    """ë„¤ì´ë²„ API ì‘ë‹µ ì•„ì´í…œì„ ì •ì œëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    return {
        "title": clean_html_tags(item.get("title", "")),
        "link": item.get("link", ""),
        "description": clean_html_tags(item.get("description", "")),
        "pubDate": format_pub_date(item.get("pubDate", "")),
        "source": extract_source_from_link(item.get("originallink", item.get("link", ""))),
    }


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜

    Note: ì‹¤ì œ API í˜¸ì¶œì€ ë„¤ì´ë²„ ê°œë°œì ì„¼í„°ì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    MCP ì„œë²„ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ ìŠ¤í¬ë¦½íŠ¸ ëŒ€ì‹  MCP ë„êµ¬ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì„¸ìš”.
    """
    print("=" * 50)
    print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 50)

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ìˆìœ¼ë©´)
    existing_data = None
    if os.path.exists(OUTPUT_PATH):
        try:
            with open(OUTPUT_PATH, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
            print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {OUTPUT_PATH}")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ í™•ì¸
    client_id = os.environ.get("NAVER_CLIENT_ID")
    client_secret = os.environ.get("NAVER_CLIENT_SECRET")

    if not client_id or not client_secret:
        print("\nâš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:")
        print("   - NAVER_CLIENT_ID")
        print("   - NAVER_CLIENT_SECRET")
        print("\nğŸ“Œ í˜„ì¬ëŠ” ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ MCPë¥¼ í†µí•´ ìˆ˜ì§‘í•˜ì„¸ìš”.")
        return

    # API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„)
    import urllib.parse
    import urllib.request

    news_data = {"last_updated": datetime.now().isoformat(), "regions": {}}

    for region in TARGET_REGIONS:
        print(f"\nğŸ” {region} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

        query = urllib.parse.quote(f"{region} ì•„íŒŒíŠ¸")
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=5&sort=date"

        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)

        try:
            response = urllib.request.urlopen(request)
            result = json.loads(response.read().decode("utf-8"))

            items = [process_news_item(item) for item in result.get("items", [])]

            news_data["regions"][region] = {
                "total_news": result.get("total", 0),
                "items": items,
                "summary": generate_summary(items, region),
            }

            print(f"   âœ… {len(items)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ê¸°ì¡´ ë°ì´í„° ìœ ì§€
            if existing_data and region in existing_data.get("regions", {}):
                news_data["regions"][region] = existing_data["regions"][region]
                print("   ğŸ“¦ ê¸°ì¡´ ë°ì´í„° ìœ ì§€")

    # ê²°ê³¼ ì €ì¥
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_PATH}")
    print(f"ğŸ“Š ì´ {len(news_data['regions'])}ê°œ ì§€ì—­ ìˆ˜ì§‘")


if __name__ == "__main__":
    main()
